#!/usr/bin/env python
"""
01_train_tkn.py (enhanced)
-------------------------
Tokenise the triplets produced by 00_build_triplets.py, build the
vocabulary, convert to ID sequences, and save everything.

This script **never touches the raw MSâ€‘MARCO split** â€“ it works only with
 the JSONL triplets file.
"""
from __future__ import annotations
import os, sys, re, pickle, argparse, collections, json, datetime
from pathlib import Path
from typing import List, Dict

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus   import stopwords
from dotenv import load_dotenv

# Install wandb / huggingfaceâ€‘hub on the fly if they are missing (handy on bare VMs)
try:
    import wandb  # type: ignore
except ModuleNotFoundError:  # pragma: no cover â€“Â runtime convenience
    os.system(f"{sys.executable} -m pip install -q wandb")
    import wandb  # type: ignore

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0ï¸âƒ£ Â Config & CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PAD_TOKEN    = "<PAD>"
TOKEN_RE     = re.compile(r"[^\w\s-]")
STOP_WORDS   = set()


def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--triplets", required=True,
                   help="JSONL produced by 00_build_triplets.py")
    p.add_argument("--top_n_words", type=int, default=50_000)
    p.add_argument("--save_dir",    default="data/tokens",
                   help="Where to write the pickle artefacts")
    p.add_argument("--wandb_project", default="msâ€‘marcoâ€‘tokeniser")
    p.add_argument("--wandb_run_name", default=None,
                   help="If set, overrides the automatic run name")
    p.add_argument("--hf_repo", default=None,
                   help="Hugging Face Hub repo name (e.g. user_name/dataset_name)."
                        " If omitted, no upload is attempted.")
    p.add_argument("--hf_repo_type", default="dataset", choices=["dataset", "model"],
                   help="Repo type on the Hub â€“Â usually 'dataset'.")
    return p.parse_args()


args = parse_args()
SAVE_DIR = Path(args.save_dir)
SAVE_DIR.mkdir(parents=True, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ Â NLTKÂ setup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for resource in ("punkt", "stopwords"):
    try:
        nltk.data.find(f"tokenizers/{resource}" if resource == "punkt" else f"corpora/{resource}")
    except LookupError:
        nltk.download(resource, quiet=True)
STOP_WORDS = set(stopwords.words("english"))


def preprocess(text: str) -> List[str]:
    """Lowerâ€‘case, strip punctuation / hyphens, tokenise, and drop stop words."""
    text = TOKEN_RE.sub(" ", text.lower()).replace("-", " ")
    return [w for w in word_tokenize(text) if w.isalpha() and w not in STOP_WORDS]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ Â Load triplets & tokenise
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"ğŸ“–  reading {args.triplets} â€¦")
query_toks, pos_toks, neg_toks = [], [], []  # type: List[List[str]]
all_tokens: List[str] = []

with open(args.triplets, encoding="utfâ€‘8") as f:
    for line in f:
        j = json.loads(line)
        q_t = preprocess(j["query"]);    query_toks.append(q_t); all_tokens.extend(q_t)
        p_t = preprocess(j["positive"]); pos_toks.append(p_t);   all_tokens.extend(p_t)
        n_t = preprocess(j["negative"]); neg_toks.append(n_t);   all_tokens.extend(n_t)

print(f"âœ…  tokenised {len(query_toks):,} triplets")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ Â Build vocabulary
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
freq        = collections.Counter(all_tokens)
most_common = [w for w, _ in freq.most_common(args.top_n_words)]
idx_to_word = [PAD_TOKEN] + most_common
word_to_idx = {w: i for i, w in enumerate(idx_to_word)}
print(f"ğŸ§   vocab size (+PAD): {len(idx_to_word):,}")


def to_ids(toks: List[List[str]]) -> List[List[int]]:
    return [[word_to_idx.get(w, 0) for w in seq] for seq in toks]


q_ids  = to_ids(query_toks)
p_ids  = to_ids(pos_toks)
n_ids  = to_ids(neg_toks)
corpus = [word_to_idx.get(w, 0) for w in all_tokens]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4ï¸âƒ£ Â Save artefacts locally
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ’¾  saving pickle artefacts â€¦")
out: Dict[str, object] = {
    "query_tokens.pkl":    query_toks,
    "positive_tokens.pkl": pos_toks,
    "negative_tokens.pkl": neg_toks,
    "query_ids.pkl":       q_ids,
    "positive_ids.pkl":    p_ids,
    "negative_ids.pkl":    n_ids,
    "word_to_idx.pkl":     word_to_idx,
    "idx_to_word.pkl":     idx_to_word,
    "corpus_ids.pkl":      corpus,
}
for fname, obj in out.items():
    with open(SAVE_DIR / fname, "wb") as f:
        pickle.dump(obj, f)
print(f"âœ…  saved {len(out)} files âœ {SAVE_DIR}\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5ï¸âƒ£ Â wandb logging
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ“Š  Initialising wandb â€¦")
load_dotenv()
# Accept both WANDB_API_KEY (official) and legacy WANDDB_KEY env names
wandb_key = os.environ.get("WANDB_API_KEY") or os.environ.get("WANDDB_KEY")
if wandb_key:
    os.environ["WANDB_API_KEY"] = wandb_key  # ensure wandb sees it
else:
    print("âš ï¸  WANDB_API_KEY not found â€“ skipping wandb logging.\n")

run = None
if os.environ.get("WANDB_API_KEY"):
    run = wandb.init(
        project=args.wandb_project,
        name=args.wandb_run_name or f"tokenise-{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}",
        config=dict(
            triplets_file=args.triplets,
            top_n_words=args.top_n_words,
            num_triplets=len(query_toks),
            vocab_size=len(idx_to_word),
        ),
    )

    # Log simple scalars
    run.log({
        "vocab_size": len(idx_to_word),
        "num_triplets": len(query_toks),
    })

    # Attach all pickle files as a datasetâ€‘type Artifact
    art = wandb.Artifact("tokeniser-output", type="dataset")
    for fname in out.keys():
        art.add_file(str(SAVE_DIR / fname))
    run.log_artifact(art)

    run.finish()
    print("âœ…  wandb run finished and artefacts logged.\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6ï¸âƒ£ Â Upload to Hugging Face Hub
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if args.hf_repo:
    print("ğŸš€  Uploading artefacts to the Hugging Face Hub â€¦")
    try:
        from huggingface_hub import HfApi, create_repo
    except ImportError:  # pragma: no cover â€“Â runtime convenience
        print("ğŸ“¦  Installing huggingfaceâ€‘hub â€¦")
        os.system(f"{sys.executable} -m pip install -q huggingface_hub")
        from huggingface_hub import HfApi, create_repo

    hf_token = os.environ.get("HUGGINGFACE_TOKEN") or os.environ.get("HUGGINGFACE_KEY")
    if not hf_token:
        print("âš ï¸  HUGGINGFACE_TOKEN not found â€“ cannot push to the Hub. Skipping.\n")
    else:
        api = HfApi()
        # Create repo if it doesn't yet exist
        create_repo(repo_id=args.hf_repo, repo_type=args.hf_repo_type, token=hf_token, exist_ok=True)

        # Craft README / model card
        readme = f"""---
license: mit
tags:
- msâ€‘marco
- tokeniser
---

# MSâ€‘MARCO Tokeniser Output

This repository stores vocabulary & ID mappings produced by `01_train_tkn.py`.

* **Triplets file**: `{args.triplets}`
* **Topâ€‘N words kept**: {args.top_n_words}
* **Vocab size (incl. PAD)**: {len(idx_to_word):,}
* **Triplets processed**: {len(query_toks):,}

The pickle files can be loaded via `pickle.load(open(fname, 'rb'))`.
"""
        Path(SAVE_DIR / "README.md").write_text(readme)

        # Which files to push?  All pickle files + README.
        files_to_upload = [SAVE_DIR / f for f in out.keys()] + [SAVE_DIR / "README.md"]

        for fp in files_to_upload:
            print(f"  â†³ uploading {fp.name}")
            api.upload_file(
                path_or_fileobj=str(fp),
                path_in_repo=fp.name,
                repo_id=args.hf_repo,
                repo_type=args.hf_repo_type,
                token=hf_token,
            )
        print(f"âœ…  Artefacts available at https://huggingface.co/{args.hf_repo}\n")
else:
    print("â„¹ï¸  --hf_repo not provided; skipping Hugging Face upload.")

print("ğŸ‰  Done!")
