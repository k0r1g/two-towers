{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54116d2c",
   "metadata": {},
   "source": [
    "# MS MARCO v1.1 Dataset Exploration\n",
    "\n",
    "This notebook provides an exploratory data analysis (EDA) of the MS MARCO v1.1 training dataset (`train.parquet`).\n",
    "\n",
    "MS MARCO is a collection of datasets focused on deep learning in search, originally released at NIPS 2016. It features real Bing questions and human-generated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b06a7",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "Based on the dataset info, MS MARCO v1.1 contains the following features:\n",
    "\n",
    "- **answers**: List of string answers\n",
    "- **passages**: List of dictionaries containing:\n",
    "  - **is_selected**: Integer flag (1 if passage was selected as relevant, 0 otherwise)\n",
    "  - **passage_text**: The text content of the passage\n",
    "  - **url**: Source URL of the passage\n",
    "- **query**: The search query text\n",
    "- **query_id**: Unique identifier for the query\n",
    "- **query_type**: Type of the query\n",
    "- **wellFormedAnswers**: List of well-formatted answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train.parquet dataset\n",
    "parquet_path = os.path.join('data', 'raw', 'parquet', 'train.parquet')\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(parquet_path):\n",
    "    print(f\"Loading dataset from {parquet_path}...\")\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(f\"Dataset loaded successfully with {len(df):,} rows.\")\n",
    "else:\n",
    "    print(f\"Error: File not found at {parquet_path}\")\n",
    "    print(\"Searching for alternative locations...\")\n",
    "    \n",
    "    # Try alternative locations\n",
    "    alt_paths = [\n",
    "        'data/parquet/train.parquet',\n",
    "        'train.parquet'\n",
    "    ]\n",
    "    \n",
    "    for path in alt_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Found dataset at {path}\")\n",
    "            df = pd.read_parquet(path)\n",
    "            print(f\"Dataset loaded successfully with {len(df):,} rows.\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"Could not find the train.parquet file. Please check the file path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19749320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "for col in df.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "print(\"\\nMemory usage:\")\n",
    "print(f\"{df.memory_usage(deep=True).sum() / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of the data to understand its structure\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f4031",
   "metadata": {},
   "source": [
    "## Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics about queries\n",
    "df['query_length'] = df['query'].apply(len)\n",
    "df['query_word_count'] = df['query'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Query length statistics (characters):\")\n",
    "print(df['query_length'].describe())\n",
    "\n",
    "print(\"\\nQuery word count statistics:\")\n",
    "print(df['query_word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize query length distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['query_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Query Lengths (characters)')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['query_word_count'], bins=30, kde=True)\n",
    "plt.title('Distribution of Query Word Counts')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00107282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze query types\n",
    "query_type_counts = df['query_type'].value_counts()\n",
    "\n",
    "print(\"Query type distribution:\")\n",
    "print(query_type_counts)\n",
    "\n",
    "# Plot query types\n",
    "plt.figure(figsize=(12, 6))\n",
    "query_type_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Query Types')\n",
    "plt.xlabel('Query Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f855b",
   "metadata": {},
   "source": [
    "## Passages Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e564c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count passages per query\n",
    "df['num_passages'] = df['passages'].apply(len)\n",
    "\n",
    "print(\"Passages per query statistics:\")\n",
    "print(df['num_passages'].describe())\n",
    "\n",
    "# Count selected passages per query (passages marked as relevant)\n",
    "df['num_selected_passages'] = df['passages'].apply(lambda x: sum(1 for p in x if p['is_selected'] == 1))\n",
    "\n",
    "print(\"\\nSelected passages per query statistics:\")\n",
    "print(df['num_selected_passages'].describe())\n",
    "print(\"\\nDistribution of selected passages per query:\")\n",
    "print(df['num_selected_passages'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize passages distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['num_passages'], bins=20, kde=False)\n",
    "plt.title('Distribution of Passages per Query')\n",
    "plt.xlabel('Number of Passages')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(y=df['num_selected_passages'])\n",
    "plt.title('Distribution of Selected Passages per Query')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Number of Selected Passages')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze passage lengths\n",
    "# This might take a while for large datasets\n",
    "def get_passage_lengths(row):\n",
    "    lengths = [len(p['passage_text']) for p in row]\n",
    "    return {\n",
    "        'min_length': min(lengths) if lengths else 0,\n",
    "        'max_length': max(lengths) if lengths else 0,\n",
    "        'avg_length': sum(lengths) / len(lengths) if lengths else 0,\n",
    "        'selected_length': next((len(p['passage_text']) for p in row if p.get('is_selected') == 1), 0)\n",
    "    }\n",
    "\n",
    "# Run on a sample to avoid long processing time\n",
    "sample_size = min(10000, len(df))\n",
    "sample_df = df.sample(sample_size, random_state=42)\n",
    "\n",
    "passage_lengths = sample_df['passages'].apply(get_passage_lengths)\n",
    "passage_stats = pd.DataFrame(passage_lengths.tolist())\n",
    "\n",
    "print(\"Passage length statistics (characters) from sample of\", sample_size, \"rows:\")\n",
    "print(passage_stats.describe())\n",
    "\n",
    "# Plot passage lengths\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(passage_stats['avg_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Average Passage Length')\n",
    "plt.xlabel('Average Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(passage_stats['selected_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Selected Passage Length')\n",
    "plt.xlabel('Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278565b",
   "metadata": {},
   "source": [
    "## Answers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze answer counts and lengths\n",
    "df['num_answers'] = df['answers'].apply(len)\n",
    "df['num_well_formed_answers'] = df['wellFormedAnswers'].apply(len)\n",
    "\n",
    "print(\"Number of answers per query:\")\n",
    "print(df['num_answers'].describe())\n",
    "print(\"\\nNumber of well-formed answers per query:\")\n",
    "print(df['num_well_formed_answers'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of number of answers\n",
    "answer_counts = pd.DataFrame({\n",
    "    'Regular Answers': df['num_answers'].value_counts().sort_index(),\n",
    "    'Well-Formed Answers': df['num_well_formed_answers'].value_counts().sort_index()\n",
    "})\n",
    "\n",
    "print(\"Distribution of answers per query:\")\n",
    "print(answer_counts)\n",
    "\n",
    "# Plot answer counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "answer_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Answers and Well-Formed Answers per Query')\n",
    "plt.xlabel('Number of Answers')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute answer lengths for a sample\n",
    "def get_answer_lengths(answers):\n",
    "    lengths = [len(a) for a in answers]\n",
    "    return {\n",
    "        'min_length': min(lengths) if lengths else 0,\n",
    "        'max_length': max(lengths) if lengths else 0,\n",
    "        'avg_length': sum(lengths) / len(lengths) if lengths else 0\n",
    "    }\n",
    "\n",
    "# Only process rows with at least one answer\n",
    "has_answers = sample_df[sample_df['num_answers'] > 0]\n",
    "answer_lengths = has_answers['answers'].apply(get_answer_lengths)\n",
    "answer_stats = pd.DataFrame(answer_lengths.tolist())\n",
    "\n",
    "print(\"Answer length statistics (characters) from sample:\")\n",
    "print(answer_stats.describe())\n",
    "\n",
    "# Plot answer lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(answer_stats['avg_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Average Answer Length')\n",
    "plt.xlabel('Average Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730685b3",
   "metadata": {},
   "source": [
    "## Relationship Between Query Type and Other Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze metrics by query type\n",
    "query_type_metrics = df.groupby('query_type').agg({\n",
    "    'query_length': 'mean',\n",
    "    'query_word_count': 'mean',\n",
    "    'num_passages': 'mean',\n",
    "    'num_selected_passages': 'mean',\n",
    "    'num_answers': 'mean',\n",
    "    'num_well_formed_answers': 'mean',\n",
    "    'query_id': 'count'\n",
    "}).rename(columns={'query_id': 'count'})\n",
    "\n",
    "print(\"Metrics by query type:\")\n",
    "query_type_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a72ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics by query type\n",
    "metrics_to_plot = ['query_length', 'query_word_count', 'num_passages', 'num_selected_passages', \n",
    "                   'num_answers', 'num_well_formed_answers']\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    sns.barplot(x=query_type_metrics.index, y=query_type_metrics[metric], ax=axes[i])\n",
    "    axes[i].set_title(f'Average {metric.replace(\"_\", \" \").title()} by Query Type')\n",
    "    axes[i].set_xlabel('Query Type')\n",
    "    axes[i].set_ylabel('Average Value')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e70989",
   "metadata": {},
   "source": [
    "## Example Queries and Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example queries and their passages\n",
    "def display_example(row):\n",
    "    print(f\"Query ID: {row['query_id']}\")\n",
    "    print(f\"Query: {row['query']}\")\n",
    "    print(f\"Query Type: {row['query_type']}\")\n",
    "    print(f\"Number of Passages: {len(row['passages'])}\")\n",
    "    \n",
    "    print(\"\\nAnswers:\")\n",
    "    for i, answer in enumerate(row['answers']):\n",
    "        print(f\"  {i+1}. {answer}\")\n",
    "    \n",
    "    if row['wellFormedAnswers']:\n",
    "        print(\"\\nWell-Formed Answers:\")\n",
    "        for i, answer in enumerate(row['wellFormedAnswers']):\n",
    "            print(f\"  {i+1}. {answer}\")\n",
    "    \n",
    "    print(\"\\nSelected Passages:\")\n",
    "    selected = [(i, p) for i, p in enumerate(row['passages']) if p['is_selected'] == 1]\n",
    "    if selected:\n",
    "        for i, passage in selected:\n",
    "            print(f\"  Passage {i+1} (URL: {passage['url']}):\")\n",
    "            print(f\"  {passage['passage_text'][:300]}...\" if len(passage['passage_text']) > 300 else passage['passage_text'])\n",
    "            print()\n",
    "    else:\n",
    "        print(\"  No passages were selected as relevant.\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Display 3 random examples\n",
    "print(\"Example Queries and Passages:\\n\")\n",
    "for _, row in df.sample(3, random_state=42).iterrows():\n",
    "    display_example(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d71196",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369103ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis of numerical features\n",
    "numeric_cols = ['query_length', 'query_word_count', 'num_passages', 'num_selected_passages', \n",
    "                'num_answers', 'num_well_formed_answers']\n",
    "\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a5aa5",
   "metadata": {},
   "source": [
    "## Summary Statistics and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "summary = {\n",
    "    'Total Queries': len(df),\n",
    "    'Query Types': len(df['query_type'].unique()),\n",
    "    'Avg Passages Per Query': df['num_passages'].mean(),\n",
    "    'Avg Selected Passages Per Query': df['num_selected_passages'].mean(),\n",
    "    'Queries with Answers (%)': (df['num_answers'] > 0).mean() * 100,\n",
    "    'Queries with Well-Formed Answers (%)': (df['num_well_formed_answers'] > 0).mean() * 100,\n",
    "    'Avg Answers Per Query': df['num_answers'].mean(),\n",
    "    'Avg Query Length (chars)': df['query_length'].mean(),\n",
    "    'Avg Query Words': df['query_word_count'].mean()\n",
    "}\n",
    "\n",
    "print(\"Dataset Summary Statistics:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value:.2f}\" if isinstance(value, float) else f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f3b0fd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has provided a comprehensive exploratory data analysis of the MS MARCO v1.1 dataset, focusing on:\n",
    "\n",
    "1. **Query analysis**: Length, word count, and query types\n",
    "2. **Passages analysis**: Distribution of passages and selected passages per query\n",
    "3. **Answers analysis**: Distribution of regular and well-formed answers\n",
    "4. **Relationship between query types and various metrics**\n",
    "5. **Example queries and passages**\n",
    "6. **Correlation analysis** between different numerical features\n",
    "\n",
    "The insights gained from this analysis can be used to better understand the MS MARCO dataset and inform subsequent modeling approaches for search relevance and question answering tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
