# Word2Vec Skipgram Configuration
# 
# Configuration that uses pretrained Word2Vec skipgram embeddings with mean pooling.
# Optimized for word-level semantics with frozen pretrained embeddings.

extends: default_config.yml

#==============================================================================
# Tokenizer Configuration
#==============================================================================
tokeniser:
  type: word                                  # Word-level tokenization
  max_len: 32                                 # Maximum number of words per sequence
  lowercase: true                             # Convert text to lowercase
  strip_punctuation: true                     # Remove punctuation during tokenization
  
#==============================================================================
# Model Architecture
#==============================================================================
# Embedding configuration  
embedding:
  type: pretrained                            # Use pretrained embeddings
  embedding_dim: 300                          # Dimension of Word2Vec embeddings
  source: word2vec-google-news-300            # Source of pretrained vectors
  trainable: false                            # Freeze embeddings during training
  
# Encoder configuration
encoder:
  arch: mean                                  # Mean pooling architecture
  hidden_dim: 256                             # Hidden layer dimension size
  tied_weights: true                          # Share weights between query and doc towers
  dropout: 0.1                                # Dropout rate for regularization
  
#==============================================================================
# Loss Function
#==============================================================================
loss:
  type: triplet                               # Triplet contrastive loss
  margin: 0.3                                 # Margin for triplet loss (larger for word embeddings)
  
#==============================================================================
# Training Parameters
#==============================================================================
batch_size: 128                               # Training batch size (smaller due to larger embeddings)
epochs: 5                                     # Number of training epochs
optimizer:
  type: adam                                  # Adam optimizer
  lr: 0.0005                                  # Learning rate (lower for pretrained embeddings)

#==============================================================================
# Logging and Metrics
#==============================================================================
use_wandb: true                               # Enable Weights & Biases logging

# W&B configuration
wandb:
  project: two-tower-retrieval                # W&B project name
  entity: azuremis                            # W&B username or team name
  run_name: word2vec_skipgram_run             # Custom run name for this configuration 